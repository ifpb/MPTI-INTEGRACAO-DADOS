{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exemplo de Integração de Dados\n",
    "\n",
    "\n",
    "### Funcionalidades demonstradas\n",
    "\n",
    "1. Mapeamento dos esquemas dos arquivos CSVs presentes no diretório data/input/users através da correlação das colunas correspondentes (seguindo o arquivo schema_mapping.json presente na pasta config);\n",
    "\n",
    "2. Conversão do formato dos arquivo integrado para um formato colunar de alta performance de leitura (Parquet).\n",
    "\n",
    "3. Deduplicação dos dados convertidos: No conjunto de dados convertidos há múltiplas entradas para um mesmo registro, variando apenas os valores de alguns dos campos entre elas. Foi necessário realizar um processo de deduplicação destes dados, a fim de apenas manter a última entrada de cada registro, usando como referência o id para identificação dos registros duplicados e a data de atualização (update_date) para definição do registro mais recente;\n",
    "\n",
    "4. Conversão do tipo dos dados deduplicados: No diretório config há um arquivo JSON de configuração (types_mapping.json), contendo os nomes dos campos e os respectivos tipos desejados de output. Utilizando esse arquivo como input, foi realizado um processo de conversão dos tipos dos campos descritos, no conjunto de dados deduplicados.\n",
    "\n",
    "### Notas gerais\n",
    "- Todas as operações foram realizadas utilizando Spark.\n",
    "\n",
    "- Cada operação utilizou como base o dataframe resultante do passo anterior, sendo persistido em arquivos Parquet.\n",
    "\n",
    "- Houve a transformação de tipos de dados em alguns campos (id, age, create_date, update_date)\n",
    "\n",
    "### Referências\n",
    "\n",
    "[1] PLASE, D.; NIEDRITE, L.; TARANOVS, R. A comparison of HDFS compact data formats: Avro versus Parquet / HDFS glaustųjų duomenų formatų palyginimas: Avro prieš Parquet. Mokslas – Lietuvos ateitis / Science – Future of Lithuania, v. 9, n. 3, p. 267-276, 4 jul. 2017."
   ],
   "metadata": {
    "id": "ZtwaYKi0segk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregamento dos arquivos de entrada e configuração"
   ],
   "metadata": {
    "id": "F-GDqwTYspyJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -P data/input/users/ https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/data/input/users/load.csv\n",
    "!wget -P data/input/users/ https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/data/input/users/load2.csv\n",
    "!wget -P config/ https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/config/schema_mapping.json\n",
    "!wget -P config/ https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/config/types_mapping_full.json"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnspHHw8qC_7",
    "outputId": "faec63b2-5f3c-47c4-818c-30123871215c",
    "ExecuteTime": {
     "start_time": "2023-08-23T00:27:28.196685Z",
     "end_time": "2023-08-23T00:27:31.625395Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-23 00:27:28--  https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/data/input/users/load.csv\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 655 [text/plain]\r\n",
      "Saving to: 'data/input/users/load.csv.1'\r\n",
      "\r\n",
      "load.csv.1          100%[===================>]     655  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-08-23 00:27:29 (20.8 MB/s) - 'data/input/users/load.csv.1' saved [655/655]\r\n",
      "\r\n",
      "--2023-08-23 00:27:29--  https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/data/input/users/load2.csv\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 446 [text/plain]\r\n",
      "Saving to: 'data/input/users/load2.csv.1'\r\n",
      "\r\n",
      "load2.csv.1         100%[===================>]     446  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-08-23 00:27:29 (42.5 MB/s) - 'data/input/users/load2.csv.1' saved [446/446]\r\n",
      "\r\n",
      "--2023-08-23 00:27:29--  https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/config/schema_mapping.json\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 74 [text/plain]\r\n",
      "Saving to: 'config/schema_mapping.json.1'\r\n",
      "\r\n",
      "schema_mapping.json 100%[===================>]      74  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-08-23 00:27:30 (6.42 MB/s) - 'config/schema_mapping.json.1' saved [74/74]\r\n",
      "\r\n",
      "--2023-08-23 00:27:30--  https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/config/types_mapping_full.json\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 182 [text/plain]\r\n",
      "Saving to: 'config/types_mapping_full.json.1'\r\n",
      "\r\n",
      "types_mapping_full. 100%[===================>]     182  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-08-23 00:27:31 (9.14 MB/s) - 'config/types_mapping_full.json.1' saved [182/182]\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instalação das dependências"
   ],
   "metadata": {
    "id": "245eHSgAsu5K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install findspark==2.0.1\n",
    "!pip install numpy==1.21.6\n",
    "!pip install pandas==1.3.5\n",
    "!pip install py4j==0.10.9.5\n",
    "!pip install pyspark==3.3.0\n",
    "!pip install python-dateutil==2.8.2\n",
    "!pip install pytz==2022.2.1\n",
    "!pip install six==1.16.0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VfHHpqmqSgU",
    "outputId": "b89d7f0b-f847-4890-aa74-5ed2d6528ea7",
    "ExecuteTime": {
     "start_time": "2023-08-23T21:13:03.297035Z",
     "end_time": "2023-08-23T21:13:34.180515Z"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark==2.0.1 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (2.0.1)\r\n",
      "Requirement already satisfied: numpy==1.21.6 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (1.21.6)\r\n",
      "Requirement already satisfied: pandas==1.3.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (1.3.5)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.5) (2022.2.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.5) (2.8.2)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.5) (1.21.6)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.16.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (0.10.9.5)\r\n",
      "Requirement already satisfied: pyspark==3.3.0 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (3.3.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from pyspark==3.3.0) (0.10.9.5)\r\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil==2.8.2) (1.16.0)\r\n",
      "Requirement already satisfied: pytz==2022.2.1 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (2022.2.1)\r\n",
      "Requirement already satisfied: six==1.16.0 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (1.16.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregamento da Configuração\n"
   ],
   "metadata": {
    "id": "AgjITH9Ds8dG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-Pg0gdjqirqx",
    "ExecuteTime": {
     "start_time": "2023-08-23T21:14:50.530476Z",
     "end_time": "2023-08-23T21:14:50.535610Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'INPUT_FILE_1': 'data/input/users/load.csv',\n",
    "    'INPUT_FILE_2': 'data/input/users/load2.csv',\n",
    "    'TYPES_MAPPING': 'config/types_mapping_full.json',\n",
    "    'SCHEMA_MAPPING': 'config/schema_mapping.json',\n",
    "    'OUTPUT_PATH': 'data/output/users.parquet',\n",
    "    'OUTPUT_PATH_CSV': 'data/output/users.csv',\n",
    "    'OUTPUT_PATH_DEDUPLICATED': 'data/output/users-deduplicated.parquet',\n",
    "    'APP_NAME': 'Demonstracao-IntegracaoDados'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inicialização do Spark"
   ],
   "metadata": {
    "id": "S8LLIX7RtK3b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(CONFIG['APP_NAME']).getOrCreate()"
   ],
   "metadata": {
    "id": "bX-12NjmtNfn",
    "ExecuteTime": {
     "start_time": "2023-08-23T21:15:45.933395Z",
     "end_time": "2023-08-23T21:15:45.944672Z"
    }
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapa 1 - Mapeamento de Esquemas e conversão de formatos"
   ],
   "metadata": {
    "id": "WIrYb31StEy9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\"\"\"\n",
    "Carrega o arquivo de entrada (load.csv), realiza mapeamento do esquema conforme o arquivo JSON (ex., types_mapping.json)\n",
    "\n",
    "@see config.py para ter acesso às configurações dos arquivos que são carregados \n",
    "\"\"\"\n",
    "def convert_data_types_and_formats():\n",
    "\n",
    "    ## Obtém a lista de campos na ordem original presente no CSV de entrada\n",
    "    fields1 = spark.read.csv(CONFIG['INPUT_FILE_1'], header=False).first()\n",
    "    fields2 = spark.read.csv(CONFIG['INPUT_FILE_2'], header=False).first()\n",
    "    fields = [f for f in fields1]\n",
    "\n",
    "    ## Carrega o JSON contendo uma lista indicando o tipo de dado das colunas mapeadas\n",
    "    type_mapping = spark.read.json(CONFIG['TYPES_MAPPING'], multiLine=True)\n",
    "    schema_mappping = spark.read.json(CONFIG['SCHEMA_MAPPING'], multiLine=True)\n",
    "\n",
    "    ## Constrói um dicionário fazendo a mescla com os dados que foram definidos no JSON\n",
    "    ## Isso é necessário para permitir que elementos do esquema definido no JSON sejam descritos em qualquer ordem\n",
    "    schema_dict = next(map(lambda row: row.asDict(), type_mapping.collect()))\n",
    "    schema_dict = _transform_dict(_create_dict(fields, schema_dict))\n",
    "    schema = StructType.fromJson(schema_dict)\n",
    "\n",
    "    ## De posse do esquema pronto a ser utilizado, é feito o carregamento dos dados do CSV\n",
    "    df1 = spark.read.csv(CONFIG['INPUT_FILE_1'], header=True, mode=\"DROPMALFORMED\", schema=schema)\n",
    "    df2 = spark.read.csv(CONFIG['INPUT_FILE_2'], header=True, inferSchema=True)\n",
    "    df2 = df2.drop(\"internal_id\")\n",
    "\n",
    "    df1.show()\n",
    "    df2.show()\n",
    "\n",
    "    ## Colunas correspondentes são mapeadas e os dois conjuntos são mesclados\n",
    "    for source, target in zip(schema_mappping.columns, schema_mappping.collect().pop()):\n",
    "        df2 = df2.withColumnRenamed(source, target)\n",
    "    df3 = df1.unionByName(df2)\n",
    "\n",
    "    ## Exibe o resultado (note que o esquema segue o que foi definido no arquivo JSON)\n",
    "    df3.show()\n",
    "    print(df3.printSchema)\n",
    "\n",
    "    ## Salva os dados carregados como Parquet no diretório indicado\n",
    "    if not os.path.isdir(CONFIG['OUTPUT_PATH']):\n",
    "        df3.write.parquet(CONFIG['OUTPUT_PATH'])\n",
    "\n",
    "def _transform_dict(d):\n",
    "    \"\"\"\n",
    "    Recebe um dicionário e retorna a versão mapeada para ser utilizada como StructField\n",
    "\n",
    "    :param d: o dicionário a ser transformado, contendo como chave o nome da coluna e como valor o tipo de dado\n",
    "    :return: versão de dicionário compatível com os campos da StructType\n",
    "    \"\"\"\n",
    "    newdict = {}\n",
    "    fields = []\n",
    "    for k,v in d.items():\n",
    "        item = {}\n",
    "        item['name'] = k\n",
    "        item['type'] = v\n",
    "        item['nullable'] = True\n",
    "        item['metadata'] = {}\n",
    "        fields.append(item)\n",
    "    newdict['fields'] = fields\n",
    "    newdict['type'] = 'struct'\n",
    "    return newdict\n",
    "\n",
    "def _create_dict(fields, schema_dict):\n",
    "    \"\"\"\"\n",
    "    Cria um dicionário a partir da lista completa de campos lidos do CSV e dos itens mapeados no JSON\n",
    "    Caso um elemento presente no CSV não seja mapeado no JSON, o seu tipo de dado é atribuído como string\n",
    "    \"\"\"\n",
    "    newdict = {}\n",
    "    for f in fields:\n",
    "        if f in schema_dict:\n",
    "            newdict[f] = schema_dict[f]\n",
    "        else:\n",
    "            newdict[f] = 'string'\n",
    "    return newdict\n",
    "\n",
    "\n",
    "convert_data_types_and_formats()"
   ],
   "metadata": {
    "id": "vO4q_RCHsBSW",
    "ExecuteTime": {
     "start_time": "2023-08-23T21:18:16.565646Z",
     "end_time": "2023-08-23T21:18:17.856834Z"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "| id|               email|           name|          phone|             address|age|         create_date|         update_date|\n",
      "+---+--------------------+---------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "|  1|david.lynch@compa...|    David Lynch|(11) 99999-9997|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-03-03 18:47:...|\n",
      "|  1|david.lynch@compa...|    David Lynch|(11) 99999-9998|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-04-14 17:09:...|\n",
      "|  2|sherlock.holmes@c...|Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|2018-04-21 20:21:...|\n",
      "|  1|david.lynch@compa...|    David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-05-23 10:13:...|\n",
      "+---+--------------------+---------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "| id|            fullname|       email_address|      telephone|             address|age|         create_date|         update_date|\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "|  3|Spongebob Squarep...|spongebob.squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 05:08:...|\n",
      "|  3|Spongebob Squarep...|spongebob.squarep...|(11) 91234-5678|124 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 04:07:...|\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "| id|               email|                name|          phone|             address|age|         create_date|         update_date|\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "|  1|david.lynch@compa...|         David Lynch|(11) 99999-9997|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-03-03 18:47:...|\n",
      "|  1|david.lynch@compa...|         David Lynch|(11) 99999-9998|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-04-14 17:09:...|\n",
      "|  2|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|2018-04-21 20:21:...|\n",
      "|  1|david.lynch@compa...|         David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-05-23 10:13:...|\n",
      "|  3|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 05:08:...|\n",
      "|  3|spongebob.squarep...|Spongebob Squarep...|(11) 91234-5678|124 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 04:07:...|\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "\n",
      "<bound method DataFrame.printSchema of DataFrame[id: int, email: string, name: string, phone: string, address: string, age: int, create_date: timestamp, update_date: timestamp]>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapa 2 - Deduplicação de Dados"
   ],
   "metadata": {
    "id": "QnzEo7C-uETn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "\"\"\"\n",
    "Recupera a lista de usuários construída no arquivo (converter.py) e realiza a remoção das instâncias duplicadas\n",
    "\"\"\"\n",
    "def deduplicate():\n",
    "    ## Carrega lista de usuários persistida no formato Parquet\n",
    "    users = spark.read.parquet(CONFIG['OUTPUT_PATH'])\n",
    "\n",
    "    ## Cria um grupo contendo um id único e a data de última atualização das instâncias vinculadas ao id corrente\n",
    "    cluster = users.groupBy('id').agg(func.max(\"update_date\").alias('update_date'))\n",
    "\n",
    "    ## Faz o join do dataframe completo com o grupo, removendo as duplicatas\n",
    "    users_deduplicated = users.join(cluster, ['id', 'update_date'])\\\n",
    "        .sort(users.id.asc())\n",
    "\n",
    "    ## Exibe o resultado\n",
    "    users_deduplicated.show()\n",
    "\n",
    "    ## Persiste o novo dataframe em um novo Parquet\n",
    "    if not os.path.isdir(CONFIG['OUTPUT_PATH_DEDUPLICATED']):\n",
    "        users_deduplicated.write.parquet(CONFIG['OUTPUT_PATH_DEDUPLICATED'])\n",
    "    return users_deduplicated\n",
    "\n",
    "deduplicate()"
   ],
   "metadata": {
    "id": "M-UL78IDtVuQ",
    "ExecuteTime": {
     "start_time": "2023-08-23T21:24:40.136918Z",
     "end_time": "2023-08-23T21:24:40.707304Z"
    }
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "| id|         update_date|               email|                name|          phone|             address|age|         create_date|\n",
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "|  1|2018-05-23 10:13:...|david.lynch@compa...|         David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|\n",
      "|  2|2018-04-21 20:21:...|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|\n",
      "|  3|2018-05-19 05:08:...|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|\n",
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataFrame[id: int, update_date: timestamp, email: string, name: string, phone: string, address: string, age: int, create_date: timestamp]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapa 3 - Operações"
   ],
   "metadata": {
    "id": "7n7C_3omud73"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def operations():\n",
    "    users = spark.read.parquet(CONFIG['OUTPUT_PATH_DEDUPLICATED'])\n",
    "    users = users.sort(users.id.asc())\n",
    "    users.show()\n",
    "    print(\"Total de usuários = \", users.count())\n",
    "    users_pd = users.toPandas()\n",
    "    print(\"Média de idade = \", users_pd['age'].mean())\n",
    "    print(\"Usuário mais velho = \", users.select('name', 'email', 'age').sort(users.age.desc()).first())\n",
    "    print(\"Usuário mais novo = \", users.select('name', 'email', 'age').sort(users.age.asc()).first())\n",
    "\n",
    "operations()"
   ],
   "metadata": {
    "id": "t2ZwlB5EuXAW",
    "ExecuteTime": {
     "start_time": "2023-08-23T21:25:55.414324Z",
     "end_time": "2023-08-23T21:25:55.771869Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "| id|         update_date|               email|                name|          phone|             address|age|         create_date|\n",
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "|  1|2018-05-23 10:13:...|david.lynch@compa...|         David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|\n",
      "|  2|2018-04-21 20:21:...|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|\n",
      "|  3|2018-05-19 05:08:...|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|\n",
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "\n",
      "Total de usuários =  3\n",
      "Média de idade =  39.666666666666664\n",
      "Usuário mais velho =  Row(name='David Lynch', email='david.lynch@company.ai', age=72)\n",
      "Usuário mais novo =  Row(name='Spongebob Squarepants', email='spongebob.squarepants@company.ai', age=13)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_FaRi3viehM8"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
