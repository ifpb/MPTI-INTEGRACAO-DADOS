{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Exemplo de Integração de Dados\n",
    "\n",
    "\n",
    "### Funcionalidades demonstradas\n",
    "\n",
    "1. Mapeamento dos esquemas dos arquivos CSVs presentes no diretório data/input/users através da correlação das colunas correspondentes (seguindo o arquivo schema_mapping.json presente na pasta config);\n",
    "\n",
    "2. Conversão do formato dos arquivo integrado para um formato colunar de alta performance de leitura (Parquet).\n",
    "\n",
    "3. Deduplicação dos dados convertidos: No conjunto de dados convertidos há múltiplas entradas para um mesmo registro, variando apenas os valores de alguns dos campos entre elas. Foi necessário realizar um processo de deduplicação destes dados, a fim de apenas manter a última entrada de cada registro, usando como referência o id para identificação dos registros duplicados e a data de atualização (update_date) para definição do registro mais recente;\n",
    "\n",
    "4. Conversão do tipo dos dados deduplicados: No diretório config há um arquivo JSON de configuração (types_mapping.json), contendo os nomes dos campos e os respectivos tipos desejados de output. Utilizando esse arquivo como input, foi realizado um processo de conversão dos tipos dos campos descritos, no conjunto de dados deduplicados.\n",
    "\n",
    "### Notas gerais\n",
    "- Todas as operações foram realizadas utilizando Spark.\n",
    "\n",
    "- Cada operação utilizou como base o dataframe resultante do passo anterior, sendo persistido em arquivos Parquet.\n",
    "\n",
    "- Houve a transformação de tipos de dados em alguns campos (id, age, create_date, update_date)\n",
    "\n",
    "### Referências\n",
    "\n",
    "[1] PLASE, D.; NIEDRITE, L.; TARANOVS, R. A comparison of HDFS compact data formats: Avro versus Parquet / HDFS glaustųjų duomenų formatų palyginimas: Avro prieš Parquet. Mokslas – Lietuvos ateitis / Science – Future of Lithuania, v. 9, n. 3, p. 267-276, 4 jul. 2017."
   ],
   "metadata": {
    "id": "ZtwaYKi0segk"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregamento dos arquivos de entrada e configuração"
   ],
   "metadata": {
    "id": "F-GDqwTYspyJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -P data/input/users/ https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/data/input/users/load.csv\n",
    "!wget -P data/input/users/ https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/data/input/users/load2.csv\n",
    "!wget -P config/ https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/config/schema_mapping.json\n",
    "!wget -P config/ https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/config/types_mapping_full.json"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TnspHHw8qC_7",
    "outputId": "faec63b2-5f3c-47c4-818c-30123871215c",
    "ExecuteTime": {
     "start_time": "2023-08-25T10:37:11.455981Z",
     "end_time": "2023-08-25T10:37:13.921691Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-08-25 10:37:11--  https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/data/input/users/load.csv\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8003::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8003::154|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 655 [text/plain]\r\n",
      "Saving to: 'data/input/users/load.csv.2'\r\n",
      "\r\n",
      "load.csv.2          100%[===================>]     655  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-08-25 10:37:12 (27.2 MB/s) - 'data/input/users/load.csv.2' saved [655/655]\r\n",
      "\r\n",
      "--2023-08-25 10:37:12--  https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/data/input/users/load2.csv\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8002::154, 2606:50c0:8001::154, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 446 [text/plain]\r\n",
      "Saving to: 'data/input/users/load2.csv.2'\r\n",
      "\r\n",
      "load2.csv.2         100%[===================>]     446  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-08-25 10:37:12 (17.7 MB/s) - 'data/input/users/load2.csv.2' saved [446/446]\r\n",
      "\r\n",
      "--2023-08-25 10:37:12--  https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/config/schema_mapping.json\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8002::154, 2606:50c0:8001::154, 2606:50c0:8003::154, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8002::154|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 74 [text/plain]\r\n",
      "Saving to: 'config/schema_mapping.json.2'\r\n",
      "\r\n",
      "schema_mapping.json 100%[===================>]      74  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-08-25 10:37:13 (5.88 MB/s) - 'config/schema_mapping.json.2' saved [74/74]\r\n",
      "\r\n",
      "--2023-08-25 10:37:13--  https://raw.githubusercontent.com/ifpb/Integracao-dados-overview/main/config/types_mapping_full.json\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8003::154, 2606:50c0:8000::154, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 182 [text/plain]\r\n",
      "Saving to: 'config/types_mapping_full.json.2'\r\n",
      "\r\n",
      "types_mapping_full. 100%[===================>]     182  --.-KB/s    in 0s      \r\n",
      "\r\n",
      "2023-08-25 10:37:13 (11.6 MB/s) - 'config/types_mapping_full.json.2' saved [182/182]\r\n",
      "\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instalação das dependências"
   ],
   "metadata": {
    "id": "245eHSgAsu5K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install findspark==2.0.1\n",
    "!pip install numpy==1.21.6\n",
    "!pip install pandas==1.3.5\n",
    "!pip install py4j==0.10.9.5\n",
    "!pip install pyspark==3.3.0\n",
    "!pip install python-dateutil==2.8.2\n",
    "!pip install pytz==2022.2.1\n",
    "!pip install six==1.16.0"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VfHHpqmqSgU",
    "outputId": "b89d7f0b-f847-4890-aa74-5ed2d6528ea7",
    "ExecuteTime": {
     "start_time": "2023-08-25T10:37:13.927860Z",
     "end_time": "2023-08-25T10:37:44.801645Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark==2.0.1 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (2.0.1)\r\n",
      "Requirement already satisfied: numpy==1.21.6 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (1.21.6)\r\n",
      "Requirement already satisfied: pandas==1.3.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (1.3.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.5) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.5) (2022.2.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.5) (1.21.6)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.16.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (0.10.9.5)\r\n",
      "Requirement already satisfied: pyspark==3.3.0 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (3.3.0)\r\n",
      "Requirement already satisfied: py4j==0.10.9.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from pyspark==3.3.0) (0.10.9.5)\r\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil==2.8.2) (1.16.0)\r\n",
      "Requirement already satisfied: pytz==2022.2.1 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (2022.2.1)\r\n",
      "Requirement already satisfied: six==1.16.0 in /Users/diegopessoa/opt/anaconda3/lib/python3.9/site-packages (1.16.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregamento da Configuração\n"
   ],
   "metadata": {
    "id": "AgjITH9Ds8dG"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-Pg0gdjqirqx",
    "ExecuteTime": {
     "start_time": "2023-08-25T10:37:44.805350Z",
     "end_time": "2023-08-25T10:37:44.807475Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'INPUT_FILE_1': 'data/input/users/load.csv',\n",
    "    'INPUT_FILE_2': 'data/input/users/load2.csv',\n",
    "    'TYPES_MAPPING': 'config/types_mapping_full.json',\n",
    "    'SCHEMA_MAPPING': 'config/schema_mapping.json',\n",
    "    'OUTPUT_PATH': 'data/output/users.parquet',\n",
    "    'OUTPUT_PATH_CSV': 'data/output/users.csv',\n",
    "    'OUTPUT_PATH_DEDUPLICATED': 'data/output/users-deduplicated.parquet',\n",
    "    'APP_NAME': 'Demonstracao-IntegracaoDados'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inicialização do Spark"
   ],
   "metadata": {
    "id": "S8LLIX7RtK3b"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(CONFIG['APP_NAME']).getOrCreate()"
   ],
   "metadata": {
    "id": "bX-12NjmtNfn",
    "ExecuteTime": {
     "start_time": "2023-08-25T10:37:44.810437Z",
     "end_time": "2023-08-25T10:37:47.633798Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/08/25 10:37:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapa 1 - Mapeamento de Esquemas e conversão de formatos"
   ],
   "metadata": {
    "id": "WIrYb31StEy9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\"\"\"\n",
    "Carrega o arquivo de entrada (load.csv), realiza mapeamento do esquema conforme o arquivo JSON (ex., types_mapping.json)\n",
    "\n",
    "@see config.py para ter acesso às configurações dos arquivos que são carregados \n",
    "\"\"\"\n",
    "def convert_data_types_and_formats():\n",
    "\n",
    "    ## De posse do esquema pronto a ser utilizado, é feito o carregamento dos dados do CSV\n",
    "    df1 = spark.read.csv(CONFIG['INPUT_FILE_1'], header=True, mode=\"DROPMALFORMED\")\n",
    "    df2 = spark.read.csv(CONFIG['INPUT_FILE_2'], header=True, inferSchema=True)\n",
    "    df2 = df2.drop(\"internal_id\")\n",
    "\n",
    "    df1.show()\n",
    "    df2.show()\n",
    "\n",
    "    df1.createOrReplaceTempView('df1')\n",
    "    df2.createOrReplaceTempView('df2')\n",
    "\n",
    "    df3 = spark.sql('SELECT email, name, phone, address, age, create_date, update_date FROM df1'\n",
    "                    ' UNION ALL '\n",
    "                    'SELECT email_address, fullname, telephone, address, age, create_date, update_date FROM df2')\n",
    "\n",
    "    df3.show()\n",
    "\n",
    "    ## Salva os dados carregados como Parquet no diretório indicado\n",
    "    if not os.path.isdir(CONFIG['OUTPUT_PATH']):\n",
    "        df3.write.parquet(CONFIG['OUTPUT_PATH'])\n",
    "\n",
    "convert_data_types_and_formats()\n"
   ],
   "metadata": {
    "id": "vO4q_RCHsBSW",
    "ExecuteTime": {
     "start_time": "2023-08-25T11:01:04.556483Z",
     "end_time": "2023-08-25T11:01:04.847690Z"
    }
   },
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "| id|               email|           name|          phone|             address|age|         create_date|         update_date|\n",
      "+---+--------------------+---------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "|  1|david.lynch@compa...|    David Lynch|(11) 99999-9997|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-03-03 18:47:...|\n",
      "|  1|david.lynch@compa...|    David Lynch|(11) 99999-9998|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-04-14 17:09:...|\n",
      "|  2|sherlock.holmes@c...|Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|2018-04-21 20:21:...|\n",
      "|  1|david.lynch@compa...|    David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-05-23 10:13:...|\n",
      "+---+--------------------+---------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "| id|            fullname|       email_address|      telephone|             address|age|         create_date|         update_date|\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "|  3|Spongebob Squarep...|spongebob.squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 05:08:...|\n",
      "|  3|Spongebob Squarep...|spongebob.squarep...|(11) 91234-5678|124 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 04:07:...|\n",
      "+---+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "\n",
      "+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "|               email|                name|          phone|             address|age|         create_date|         update_date|\n",
      "+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "|david.lynch@compa...|         David Lynch|(11) 99999-9997|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-03-03 18:47:...|\n",
      "|david.lynch@compa...|         David Lynch|(11) 99999-9998|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-04-14 17:09:...|\n",
      "|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|2018-04-21 20:21:...|\n",
      "|david.lynch@compa...|         David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|2018-05-23 10:13:...|\n",
      "|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 05:08:...|\n",
      "|spongebob.squarep...|Spongebob Squarep...|(11) 91234-5678|124 Conch Street,...| 13|2018-05-19 04:07:...|2018-05-19 04:07:...|\n",
      "+--------------------+--------------------+---------------+--------------------+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapa 2 - Deduplicação de Dados"
   ],
   "metadata": {
    "id": "QnzEo7C-uETn"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pyspark.sql.functions as func\n",
    "\n",
    "\"\"\"\n",
    "Recupera a lista de usuários construída no arquivo (converter.py) e realiza a remoção das instâncias duplicadas\n",
    "\"\"\"\n",
    "def deduplicate():\n",
    "    ## Carrega lista de usuários persistida no formato Parquet\n",
    "    users = spark.read.parquet(CONFIG['OUTPUT_PATH'])\n",
    "\n",
    "    users.createOrReplaceTempView('users')\n",
    "\n",
    "    users_deduplicated = spark.sql('SELECT id, max(update_date), max(email) as email, max(name) as name, max(phone) as phone, max(age) as age, max(create_date) as create_date FROM users GROUP BY id');\n",
    "\n",
    "    users_deduplicated.show()\n",
    "\n",
    "    ## Persiste o novo dataframe em um novo Parquet\n",
    "    if not os.path.isdir(CONFIG['OUTPUT_PATH_DEDUPLICATED']):\n",
    "        users_deduplicated.write.parquet(CONFIG['OUTPUT_PATH_DEDUPLICATED'])\n",
    "    return users_deduplicated\n",
    "\n",
    "deduplicate()"
   ],
   "metadata": {
    "id": "M-UL78IDtVuQ",
    "ExecuteTime": {
     "start_time": "2023-08-25T11:07:46.594852Z",
     "end_time": "2023-08-25T11:07:46.731280Z"
    }
   },
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+---------------+---+--------------------+\n",
      "| id|    max(update_date)|               email|                name|          phone|age|         create_date|\n",
      "+---+--------------------+--------------------+--------------------+---------------+---+--------------------+\n",
      "|  1|2018-05-23 10:13:...|david.lynch@compa...|         David Lynch|(11) 99999-9999| 72|2018-03-03 18:47:...|\n",
      "|  2|2018-04-21 20:21:...|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623| 34|2018-04-21 20:21:...|\n",
      "|  3|2018-05-19 05:08:...|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321| 13|2018-05-19 04:07:...|\n",
      "+---+--------------------+--------------------+--------------------+---------------+---+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "DataFrame[id: int, max(update_date): timestamp, email: string, name: string, phone: string, age: int, create_date: timestamp]"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Etapa 3 - Operações"
   ],
   "metadata": {
    "id": "7n7C_3omud73"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def operations():\n",
    "    users = spark.read.parquet(CONFIG['OUTPUT_PATH_DEDUPLICATED'])\n",
    "    users = users.sort(users.id.asc())\n",
    "    users.show()\n",
    "    print(\"Total de usuários = \", users.count())\n",
    "    users_pd = users.toPandas()\n",
    "    print(\"Média de idade = \", users_pd['age'].mean())\n",
    "    print(\"Usuário mais velho = \", users.select('name', 'email', 'age').sort(users.age.desc()).first())\n",
    "    print(\"Usuário mais novo = \", users.select('name', 'email', 'age').sort(users.age.asc()).first())\n",
    "\n",
    "operations()"
   ],
   "metadata": {
    "id": "t2ZwlB5EuXAW",
    "ExecuteTime": {
     "start_time": "2023-08-25T11:07:51.294932Z",
     "end_time": "2023-08-25T11:07:52.646970Z"
    }
   },
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "| id|         update_date|               email|                name|          phone|             address|age|         create_date|\n",
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "|  1|2018-05-23 10:13:...|david.lynch@compa...|         David Lynch|(11) 99999-9999|Mulholland Drive,...| 72|2018-03-03 18:47:...|\n",
      "|  2|2018-04-21 20:21:...|sherlock.holmes@c...|     Sherlock Holmes|(11) 94815-1623|221B Baker Street...| 34|2018-04-21 20:21:...|\n",
      "|  3|2018-05-19 05:08:...|spongebob.squarep...|Spongebob Squarep...|(11) 98765-4321|122 Conch Street,...| 13|2018-05-19 04:07:...|\n",
      "+---+--------------------+--------------------+--------------------+---------------+--------------------+---+--------------------+\n",
      "\n",
      "Total de usuários =  3\n",
      "Média de idade =  39.666666666666664\n",
      "Usuário mais velho =  Row(name='David Lynch', email='david.lynch@company.ai', age=72)\n",
      "Usuário mais novo =  Row(name='Spongebob Squarepants', email='spongebob.squarepants@company.ai', age=13)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "_FaRi3viehM8"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
